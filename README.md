# ðŸ©º Report Analyzer Agent

A modular AI-based medical report analyzer that simulates specialist doctors (Cardiologist, Psychologist, Pulmonologist) and a Multidisciplinary Team (MDT) to provide expert assessments using LLMs. Built with LangChain, Streamlit, and runs locally using open-source LLMs via Ollama.

---

## ðŸš€ Features

- ðŸ“‚ Upload `.txt` medical reports through a web UI
- ðŸ§  Role-based simulated analysis by:
  - Cardiologist
  - Psychologist
  - Pulmonologist
  - Multidisciplinary Team (combined review)
- ðŸ’¬ Returns structured, human-readable diagnostic insights
- ðŸ’¾ Automatically saves analysis in `results/` folder using the patient's name
- ðŸŒ Streamlit-based browser UI (no need to write code)
- ðŸ”“ Entirely local & private using [Ollama](https://ollama.com/) + open-source LLMs

---

## ðŸ§° Tech Stack

- Python 3.10+
- LangChain
- Streamlit
- Ollama (local LLM runtime)
- LLaMA 3 / Mistral / Phi-3 (or any other compatible model)

---

## ðŸ“ Project Structure

```
ReportAnalyzerAgent/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py         # Core Agent class with prompt logic
â”‚   â””â”€â”€ roles.py              # Role-specific agent subclasses
â”œâ”€â”€ Medical reports/          # Optional folder for test report files
â”œâ”€â”€ results/                  # Outputs saved with patient-based filenames
â”œâ”€â”€ app.py                    # Streamlit UI entry point
â”œâ”€â”€ main.py                   # CLI version for local testing
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Project info
```

---

## ðŸ–¥ï¸ Local Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/YOUR_USERNAME/report-analyzer-agent.git
cd report-analyzer-agent
```

### 2. Create a virtual environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Install and run Ollama

Download from: https://ollama.com  
Then run:

```bash
ollama pull llama3
ollama run llama3
```

> Make sure Ollama is running before starting the app.

---

## ðŸ§ª Run the Application

### Option A: Streamlit UI (Recommended)

```bash
streamlit run app.py
```

- Upload a `.txt` report
- Click **Analyze Report**
- View structured results in-browser

### Option B: Command Line

Place your `.txt` file in `Medical reports/` and run:

```bash
python main.py
```

---

## ðŸ“¤ Output

- Results are saved to the `results/` folder
- Filenames are auto-generated based on patient name
- Example: `results/Michael_Johnson.txt`

---

## ðŸ“Œ Sample Input Format

Your input `.txt` should contain fields like:

```
Name: Michael Johnson
Age: 29
Gender: Male
Chief Complaint: ...
Medical History: ...
Lab Results: ...
```

---

## ðŸŒ Deployment

You can deploy this app on:
- **Streamlit Cloud** *(requires replacing local LLM with OpenAI or OpenRouter model)*
- **Hugging Face Spaces** *(using Gradio + Transformers)*

> Ollama-based local models only run on your machine â€” they are not cloud-hosted.

---

## ðŸ’¡ Future Enhancements

- Add more specialist roles (Neurologist, Endocrinologist)
- Parse PDFs or medical imaging reports
- Export results as PDFs
- Deploy cloud version with OpenRouter LLM

---

## ðŸ“œ License

This project is licensed under the MIT License.

---

## ðŸ™Œ Credits

- [LangChain](https://github.com/langchain-ai/langchain)
- [Ollama](https://ollama.com)
- [Streamlit](https://streamlit.io)
